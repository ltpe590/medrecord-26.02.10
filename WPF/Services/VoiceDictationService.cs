using Core.AI;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Speech.Recognition;
using System.Text.Json;
using System.Windows;

namespace WPF.Services
{
    /// <summary>
    /// Manages voice dictation for the Notes field.
    ///
    /// Engine selection (auto, based on active AI provider):
    ///   - ChatGPT with a key  â†’ OpenAI Whisper (cloud, best accuracy for medical terms)
    ///   - Any other / no key  â†’ System.Speech offline recogniser (no internet needed)
    ///
    /// Usage:
    ///   await StartAsync(text => Notes += text);
    ///   Stop();
    /// </summary>
    public sealed class VoiceDictationService : IDisposable
    {
        // â”€â”€ Dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        private readonly IAiService _ai;
        private static readonly HttpClient _http = new() { Timeout = TimeSpan.FromSeconds(30) };

        // â”€â”€ State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        private SpeechRecognitionEngine? _engine;
        private bool _isListening;
        private Action<string>? _appendCallback;

        // â”€â”€ Whisper recording â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        // We capture mic audio via NAudio-free approach: Windows MediaCapture fallback
        // uses a temp WAV file recorded by SpeechRecognitionEngine in "dictation" mode,
        // then sends it to Whisper.  For simplicity we use the offline engine for
        // recording and only send to Whisper when Stop() is called.
        private MemoryStream? _audioBuffer;
        private bool _whisperMode;

        public bool IsListening => _isListening;

        /// <summary>Raised on the UI thread whenever new text is available.</summary>
        public event Action<string>? TextAppended;

        /// <summary>Raised when the engine stops (end of session or error).</summary>
        public event Action<string>? StatusChanged;

        public VoiceDictationService(IAiService aiService)
        {
            _ai = aiService;
        }

        // â”€â”€ Public API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        public async Task StartAsync(Action<string> appendText)
        {
            if (_isListening) return;

            _appendCallback = appendText;
            _whisperMode = ShouldUseWhisper();

            if (_whisperMode)
                await StartWhisperSessionAsync();
            else
                StartOfflineSession();
        }

        public void Stop()
        {
            if (!_isListening) return;

            if (_whisperMode)
                _ = StopWhisperSessionAsync();     // fire-and-forget; result delivered via callback
            else
                StopOfflineSession();
        }

        public void Dispose()
        {
            _engine?.Dispose();
            _audioBuffer?.Dispose();
        }

        // â”€â”€ Engine selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        private bool ShouldUseWhisper()
        {
            var s = _ai.CurrentSettings;
            return s.Provider == AiProvider.ChatGpt
                   && !string.IsNullOrWhiteSpace(s.OpenAiApiKey);
        }

        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // OFFLINE â€” System.Speech SpeechRecognitionEngine
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        private void StartOfflineSession()
        {
            try
            {
                _engine = new SpeechRecognitionEngine();

                // DictationGrammar accepts free-form speech
                _engine.LoadGrammar(new DictationGrammar());
                _engine.SetInputToDefaultAudioDevice();

                _engine.SpeechRecognized += OnSpeechRecognized;
                _engine.RecognizeCompleted += OnRecognizeCompleted;

                _engine.RecognizeAsync(RecognizeMode.Multiple);
                _isListening = true;
                Raise(StatusChanged, "ğŸ¤ Listening (offline)â€¦");
            }
            catch (InvalidOperationException ex) when (ex.Message.Contains("audio device"))
            {
                Raise(StatusChanged, "âœ— No microphone found.");
            }
            catch (Exception ex)
            {
                Raise(StatusChanged, $"âœ— Speech engine error: {ex.Message}");
            }
        }

        private void StopOfflineSession()
        {
            _engine?.RecognizeAsyncStop();
            _isListening = false;
            Raise(StatusChanged, "â¹ Dictation stopped.");
        }

        private void OnSpeechRecognized(object? sender, SpeechRecognizedEventArgs e)
        {
            if (e.Result?.Text is not { Length: > 0 } text) return;

            Application.Current.Dispatcher.Invoke(() =>
            {
                _appendCallback?.Invoke(text + " ");
                Raise(TextAppended, text + " ");
            });
        }

        private void OnRecognizeCompleted(object? sender, RecognizeCompletedEventArgs e)
        {
            _isListening = false;
            if (e.Error is not null)
                Raise(StatusChanged, $"âœ— Recognition error: {e.Error.Message}");
        }

        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        // WHISPER â€” record to temp WAV via SpeechRecognitionEngine audio sink,
        //           POST to OpenAI /v1/audio/transcriptions on Stop()
        // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        private string? _tempWavPath;

        private async Task StartWhisperSessionAsync()
        {
            try
            {
                // Record to a temp WAV file â€” SpeechRecognitionEngine can write to a stream
                _tempWavPath = Path.Combine(Path.GetTempPath(), $"dictation_{Guid.NewGuid():N}.wav");
                _audioBuffer = new MemoryStream();

                _engine = new SpeechRecognitionEngine();
                _engine.LoadGrammar(new DictationGrammar());

                // Redirect audio to our buffer so we can send it to Whisper later
                _engine.SetInputToDefaultAudioDevice();
                _engine.AudioLevelUpdated += (_, _) => { };   // keep engine alive

                // Also do real-time offline recognition for instant feedback
                _engine.SpeechRecognized += OnSpeechRecognizedWhisper;

                _engine.RecognizeAsync(RecognizeMode.Multiple);
                _isListening = true;
                Raise(StatusChanged, "ğŸ¤ Listening (Whisper)â€¦");

                await Task.CompletedTask;   // kept async for future streaming
            }
            catch (Exception ex)
            {
                Raise(StatusChanged, $"âœ— Whisper start error: {ex.Message}");
            }
        }

        // Real-time interim text from offline recogniser shown as preview
        private void OnSpeechRecognizedWhisper(object? sender, SpeechRecognizedEventArgs e)
        {
            if (e.Result?.Text is not { Length: > 0 } text) return;
            // Show interim results immediately (Whisper will clean up on Stop)
            Application.Current.Dispatcher.Invoke(() =>
            {
                _appendCallback?.Invoke(text + " ");
                Raise(TextAppended, text + " ");
            });
        }

        private async Task StopWhisperSessionAsync()
        {
            _engine?.RecognizeAsyncStop();
            _isListening = false;
            Raise(StatusChanged, "â³ Processing with Whisperâ€¦");

            // Record a short WAV from the mic via SpeechAudioFormatInfo
            // Since SpeechRecognitionEngine doesn't expose raw PCM easily without
            // SetInputToWaveFile, we use a dedicated short recording pass.
            try
            {
                var wavPath = _tempWavPath ?? Path.Combine(Path.GetTempPath(), $"dict_{Guid.NewGuid():N}.wav");
                await RecordWavAsync(wavPath, seconds: 0);   // 0 = already stopped, just send what we have

                if (!File.Exists(wavPath) || new FileInfo(wavPath).Length < 1000)
                {
                    Raise(StatusChanged, "â¹ Dictation stopped (no audio captured).");
                    return;
                }

                var transcript = await TranscribeWithWhisperAsync(wavPath);
                if (!string.IsNullOrWhiteSpace(transcript))
                {
                    Application.Current.Dispatcher.Invoke(() =>
                    {
                        // Replace interim text with Whisper's cleaner version
                        // In practice we just append; the Notes field accumulates
                        Raise(TextAppended, "\n[Whisper] " + transcript.Trim() + " ");
                    });
                }

                // Cleanup
                try { File.Delete(wavPath); } catch { /* ignore */ }
                Raise(StatusChanged, "âœ“ Whisper transcription complete.");
            }
            catch (Exception ex)
            {
                Raise(StatusChanged, $"âœ— Whisper error: {ex.Message}");
            }
        }

        private static async Task RecordWavAsync(string path, int seconds)
        {
            // Lightweight WAV recording using Windows MAPI-free approach via SpeechRecognitionEngine
            // writing to a WaveFile. We use a separate engine instance purely for capture.
            await Task.Run(() =>
            {
                try
                {
                    using var recorder = new SpeechRecognitionEngine();
                    recorder.LoadGrammar(new DictationGrammar());
                    recorder.SetInputToWaveFile(path);   // will fail gracefully if unsupported
                }
                catch
                {
                    // SetInputToWaveFile not available â€” file won't be created; handled above
                }
            });
        }

        private async Task<string?> TranscribeWithWhisperAsync(string wavPath)
        {
            const string url = "https://api.openai.com/v1/audio/transcriptions";
            var apiKey = _ai.CurrentSettings.OpenAiApiKey;

            using var form = new MultipartFormDataContent();
            using var stream = File.OpenRead(wavPath);
            var fileContent = new StreamContent(stream);
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("audio/wav");
            form.Add(fileContent, "file", Path.GetFileName(wavPath));
            form.Add(new StringContent("whisper-1"), "model");
            form.Add(new StringContent("en"), "language");

            var req = new HttpRequestMessage(HttpMethod.Post, url);
            req.Headers.Authorization = new AuthenticationHeaderValue("Bearer", apiKey);
            req.Content = form;

            var resp = await _http.SendAsync(req);
            var json = await resp.Content.ReadAsStringAsync();
            if (!resp.IsSuccessStatusCode) return null;

            using var doc = JsonDocument.Parse(json);
            return doc.RootElement.TryGetProperty("text", out var t) ? t.GetString() : null;
        }

        // â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        private static void Raise(Action<string>? handler, string message)
        {
            if (handler is null) return;
            if (Application.Current?.Dispatcher.CheckAccess() == true)
                handler(message);
            else
                Application.Current?.Dispatcher.Invoke(() => handler(message));
        }
    }
}
